import pandas as pd
import numpy as np

dataset = pd.DataFrame({
    'outlook': ['sunny','sunny','overcast','rain','rain','rain','overcast',
                'sunny','sunny','rain','sunny','overcast','overcast','rain'],
    'temperature': ['hot','hot','hot','mild','cool','cool','mild',
                    'cool','mild','mild','mild','mild','hot','mild'],
    'humidity': ['high','high','high','high','normal','normal','normal',
                 'high','normal','normal','high','normal','high','normal'],
    'wind': ['weak','strong','weak','weak','weak','strong','strong',
             'weak','weak','weak','strong','strong','weak','strong'],
    'class': ['no','no','yes','yes','yes','no','yes',
              'no','yes','yes','no','yes','yes','no']
})


# Entropy function
def entropy(target_col):
    elements, counts = np.unique(target_col, return_counts=True)
    entropy_value = np.sum([
        -(counts[i] / np.sum(counts)) * np.log2(counts[i] / np.sum(counts))
        for i in range(len(elements))
    ])
    return entropy_value

# Information Gain function
def InfoGain(data, split_attribute_name, target_name="class"):
    total_entropy = entropy(data[target_name])

    vals, counts = np.unique(data[split_attribute_name], return_counts=True)

    weighted_entropy = np.sum([
        (counts[i] / np.sum(counts)) *
        entropy(
            data.where(data[split_attribute_name] == vals[i])
            .dropna()[target_name]
        )
        for i in range(len(vals))
    ])

    information_gain = total_entropy - weighted_entropy
    return information_gain

# ID3 Algorithm
def ID3(data, originaldata, features, target_attribute_name="class", parent_node_class=None):

    # If all target values are the same
    if len(np.unique(data[target_attribute_name])) <= 1:
        return np.unique(data[target_attribute_name])[0]

    # If dataset is empty
    elif len(data) == 0:
        return np.unique(originaldata[target_attribute_name])[
            np.argmax(np.unique(originaldata[target_attribute_name], return_counts=True)[1])
        ]

    # If no features left
    elif len(features) == 0:
        return parent_node_class

    # Otherwise
    else:
        parent_node_class = np.unique(data[target_attribute_name])[
            np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])
        ]

        # Select feature with highest information gain
        item_values = [InfoGain(data, feature, target_attribute_name) for feature in features]
        best_feature_index = np.argmax(item_values)
        best_feature = features[best_feature_index]

        tree = {best_feature: {}}

        # Remove chosen feature
        features = [f for f in features if f != best_feature]

        # Grow tree
        for value in np.unique(data[best_feature]):
            sub_data = data.where(data[best_feature] == value).dropna()
            subtree = ID3(
                sub_data,
                originaldata,
                features,
                target_attribute_name,
                parent_node_class
            )
            tree[best_feature][value] = subtree

        return tree

# Build and display tree
tree = ID3(dataset, dataset, dataset.columns[:-1])
print("\nDecision Tree:\n", tree)



output:


Decision Tree:
 {'outlook': {'overcast': 'yes', 'rain': {'wind': {'strong': 'no', 'weak': 'yes'}}, 'sunny': {'humidity': {'high': 'no', 'normal': 'yes'}}}}

